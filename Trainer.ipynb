{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependancies"
      ],
      "metadata": {
        "id": "5MQQ88E7Logy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch datasets rouge-score sacrebleu grapheme"
      ],
      "metadata": {
        "id": "6mMFWw7_HcBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "p7JLdKplWBOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from rouge_score import rouge_scorer\n",
        "import grapheme\n",
        "import seaborn as sns\n",
        "from sacrebleu.metrics import BLEU\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import Trainer, TrainingArguments, default_data_collator, EarlyStoppingCallback, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from transformers import TrainerCallback\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-09-21T16:29:45.678458Z",
          "iopub.execute_input": "2021-09-21T16:29:45.678760Z",
          "iopub.status.idle": "2021-09-21T16:29:45.903262Z",
          "shell.execute_reply.started": "2021-09-21T16:29:45.678728Z",
          "shell.execute_reply": "2021-09-21T16:29:45.902533Z"
        },
        "trusted": true,
        "id": "gJEn6QgSWBOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t_fgvn_cWLLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load datasets"
      ],
      "metadata": {
        "id": "x1yVm53xKfOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full = pd.read_csv('/content/drive/MyDrive/Research_folder/data/train.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/Research_folder/data/dev.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Research_folder/data/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:41.482769Z",
          "iopub.execute_input": "2021-09-21T16:24:41.483072Z",
          "iopub.status.idle": "2021-09-21T16:24:42.337184Z",
          "shell.execute_reply.started": "2021-09-21T16:24:41.483040Z",
          "shell.execute_reply": "2021-09-21T16:24:42.336474Z"
        },
        "trusted": true,
        "id": "siNyJ09nWBOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# percentage = 0.75\n",
        "# size = 100\n",
        "# train = train_full.sample(frac=percentage, random_state=42)  # random_state for reproducibility\n",
        "# print(f\"Loaded {len(train)} rows ({percentage*100}% of {len(train_full)} total)\")"
      ],
      "metadata": {
        "id": "B9kA-tLlGhTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translated training set\n",
        "train.head(4)"
      ],
      "metadata": {
        "id": "lZMnzK5JI-ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translated validation set\n",
        "val.head(4)"
      ],
      "metadata": {
        "id": "No36xV2DsmtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translated test set\n",
        "test.head(4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:42.338279Z",
          "iopub.execute_input": "2021-09-21T16:24:42.338532Z",
          "iopub.status.idle": "2021-09-21T16:24:42.357688Z",
          "shell.execute_reply.started": "2021-09-21T16:24:42.338498Z",
          "shell.execute_reply": "2021-09-21T16:24:42.356943Z"
        },
        "trusted": true,
        "id": "FRExrBWYWBOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Translated Train set shape:{train.shape}\")\n",
        "print(f\"Translated Validation set shape:{val.shape}\")\n",
        "print(f\"Translated Test set shape:{test.shape}\")"
      ],
      "metadata": {
        "id": "pP7qOTMnnxxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading"
      ],
      "metadata": {
        "id": "drQN6LGdKbsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "LbDuOgDHW_QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "metadata": {
        "id": "uE6BqynaLtiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "Nc1jqPQfWBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up configuration for the tokenization\n",
        "batch_size = 16\n",
        "max_length = 512\n",
        "doc_stride = 128  # Stride value to handle document overflow\n",
        "pad_on_right = tokenizer.padding_side == \"right\"  # Check if padding is applied on the right side"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:43.135485Z",
          "iopub.execute_input": "2021-09-21T16:24:43.135758Z",
          "iopub.status.idle": "2021-09-21T16:24:43.139785Z",
          "shell.execute_reply.started": "2021-09-21T16:24:43.135725Z",
          "shell.execute_reply": "2021-09-21T16:24:43.139112Z"
        },
        "trusted": true,
        "id": "onJ0sYftWBOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Remove unnecessary left whitespace from questions to avoid truncation issues\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize the examples, applying truncation, padding, and managing overflows with stride\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Map features back to their corresponding examples\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Initialize lists to store start and end positions for answers\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # Find the CLS token index for unanswered examples\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If no answer, label with CLS token index\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Calculate start and end token indices for the answer span\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Find the start and end token indices within the current span\n",
        "            token_start_index = next(i for i, s in enumerate(sequence_ids) if s == (1 if pad_on_right else 0))\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Label with CLS index if answer is out of span\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Adjust start and end positions based on answer span\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:43.141503Z",
          "iopub.execute_input": "2021-09-21T16:24:43.141912Z",
          "iopub.status.idle": "2021-09-21T16:24:43.157785Z",
          "shell.execute_reply.started": "2021-09-21T16:24:43.141855Z",
          "shell.execute_reply": "2021-09-21T16:24:43.157116Z"
        },
        "trusted": true,
        "id": "osOiv84pWBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_answers(r):\n",
        "    # Extract the starting position\n",
        "    start = r[0]\n",
        "    # Extract the answer text\n",
        "    text = r[1]\n",
        "    # Return the answer in a dictionary\n",
        "    return {\n",
        "        'answer_start': [start],  # Start index of the answer as a list.\n",
        "        'text': [text]           # The text of the answer as a list.\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:43.158624Z",
          "iopub.execute_input": "2021-09-21T16:24:43.158882Z",
          "iopub.status.idle": "2021-09-21T16:24:43.314629Z",
          "shell.execute_reply.started": "2021-09-21T16:24:43.158837Z",
          "shell.execute_reply": "2021-09-21T16:24:43.313925Z"
        },
        "trusted": true,
        "id": "IHbYHEjgWBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sample(frac=1, random_state=42)\n",
        "valid = val.sample(frac=1, random_state=42)\n",
        "\n",
        "# Combines 'answer_start' and 'answer_text' into a structured dictionary.\n",
        "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n",
        "valid['answers'] = valid[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n",
        "\n",
        "# Convert DataFrame into Hugging Face's Dataset\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "valid_dataset = Dataset.from_pandas(valid)"
      ],
      "metadata": {
        "id": "srIdsbM9Awmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:43.315964Z",
          "iopub.execute_input": "2021-09-21T16:24:43.316223Z",
          "iopub.status.idle": "2021-09-21T16:24:43.325216Z",
          "shell.execute_reply.started": "2021-09-21T16:24:43.316190Z",
          "shell.execute_reply": "2021-09-21T16:24:43.324361Z"
        },
        "trusted": true,
        "id": "I4PbX55fWBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "tokenized_train_ds = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_valid_ds = valid_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:24:43.326988Z",
          "iopub.execute_input": "2021-09-21T16:24:43.327245Z",
          "iopub.status.idle": "2021-09-21T16:25:19.111191Z",
          "shell.execute_reply.started": "2021-09-21T16:24:43.327213Z",
          "shell.execute_reply": "2021-09-21T16:25:19.110490Z"
        },
        "trusted": true,
        "id": "5jOO9Rt7WBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process"
      ],
      "metadata": {
        "id": "04Mv6x7pKRYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComputeMetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.epoch_train_losses = []  # Stores last loss of each epoch\n",
        "        self.current_epoch_train_losses = []  # Temporary storage\n",
        "        self.val_losses = []\n",
        "        self.completed_epochs = 0\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Reset training loss tracking at start of each epoch\"\"\"\n",
        "        self.current_epoch_train_losses = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is None:\n",
        "            return\n",
        "\n",
        "        # Track all training losses during epoch\n",
        "        if \"loss\" in logs:\n",
        "            self.current_epoch_train_losses.append(logs[\"loss\"])\n",
        "\n",
        "        # Capture validation metrics at epoch end\n",
        "        if \"eval_loss\" in logs:\n",
        "            self.val_losses.append(logs[\"eval_loss\"])\n",
        "        if \"eval_accuracy\" in logs:\n",
        "            self.val_accuracies.append(logs[\"eval_accuracy\"])\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Store the final training loss/accuracy of the completed epoch\"\"\"\n",
        "        if self.current_epoch_train_losses:\n",
        "            # Store last training loss of the epoch\n",
        "            self.epoch_train_losses.append(self.current_epoch_train_losses[-1])\n",
        "\n",
        "            # Optional: Calculate epoch-level training accuracy if available\n",
        "            if hasattr(state, 'train_metrics') and 'accuracy' in state.train_metrics:\n",
        "                self.train_accuracies.append(state.train_metrics['accuracy'])\n",
        "\n",
        "        self.completed_epochs += 1"
      ],
      "metadata": {
        "id": "sXSlt5luVzy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize callback\n",
        "metrics_callback = ComputeMetricsCallback()\n",
        "\n",
        "# Define training arguments\n",
        "%env WANDB_DISABLED=True\n",
        "args = TrainingArguments(\n",
        "    f\"sinhala-qa\",\n",
        "    save_total_limit=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")"
      ],
      "metadata": {
        "id": "7nsiwpaU1YEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the data collator\n",
        "data_collator = default_data_collator\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train_ds,\n",
        "    eval_dataset=tokenized_valid_ds,\n",
        "    data_collator=default_data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[early_stopping, metrics_callback],  # Add accuracy tracking\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:30:52.844172Z",
          "iopub.execute_input": "2021-09-21T16:30:52.844501Z",
          "iopub.status.idle": "2021-09-21T16:31:32.806435Z",
          "shell.execute_reply.started": "2021-09-21T16:30:52.844468Z",
          "shell.execute_reply": "2021-09-21T16:31:32.805580Z"
        },
        "trusted": true,
        "id": "SwmdB0uPWBOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # Start training the model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T16:44:21.272960Z",
          "iopub.execute_input": "2021-09-21T16:44:21.273697Z",
          "iopub.status.idle": "2021-09-21T17:17:33.003825Z",
          "shell.execute_reply.started": "2021-09-21T16:44:21.273661Z",
          "shell.execute_reply": "2021-09-21T17:17:33.002905Z"
        },
        "trusted": true,
        "id": "NpNCn_pfWBOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save metrics\n",
        "def save_metrics(metrics_data, filename):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(metrics_data, f)\n",
        "\n",
        "# Function to load metrics\n",
        "def load_metrics(filename=\"training_metrics/metrics.json\"):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return None"
      ],
      "metadata": {
        "id": "5f8--E3LHSH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metrics\n",
        "metrics_data = {\n",
        "    \"train_losses\": metrics_callback.epoch_train_losses,\n",
        "    \"val_losses\": metrics_callback.val_losses,\n",
        "    \"train_accuracies\": metrics_callback.train_accuracies,\n",
        "    \"val_accuracies\": metrics_callback.val_accuracies,\n",
        "}"
      ],
      "metadata": {
        "id": "iAi2hLcaFiBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(train_losses, val_losses, save_path=None):\n",
        "    \"\"\"\n",
        "    Plots training and validation losses with aligned epochs.\n",
        "    Assumes train_losses contains multiple values per epoch,\n",
        "    while val_losses contains one value per epoch.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Calculate how many training logs per epoch\n",
        "    logs_per_epoch = len(train_losses) // len(val_losses)\n",
        "\n",
        "    # Extract the last training loss of each epoch\n",
        "    epoch_train_losses = [train_losses[(i+1)*logs_per_epoch - 1]\n",
        "                         for i in range(len(val_losses))]\n",
        "\n",
        "    # Plot with aligned epochs\n",
        "    epochs = range(1, len(val_losses)+1)\n",
        "    plt.plot(epochs, epoch_train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='o')\n",
        "\n",
        "    plt.xticks(epochs)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Training/Validation Loss (Epochs 1-{len(val_losses)})\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train_accuracies, val_accuracies, save_path=None):\n",
        "    \"\"\"\n",
        "    Plots training and validation accuracies.\n",
        "    Assumes both are logged at the same frequency (per epoch).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    epochs = range(1, len(val_accuracies)+1)\n",
        "\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", marker='o')\n",
        "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
        "\n",
        "    plt.xticks(epochs)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"Training/Validation Accuracy (Epochs 1-{len(val_accuracies)})\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2pv4aGlMUzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path_loss=\"/content/drive/MyDrive/Research_folder/outputs/Results/XLM-R-50-metrics_loss_plot.png\""
      ],
      "metadata": {
        "id": "UCzEnUfqJNc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check exact values at key epochs\n",
        "print(f\"Final Training Loss: {metrics_callback.epoch_train_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {metrics_callback.val_losses[-1]:.4f}\")\n",
        "print(f\"Gap: {metrics_callback.val_losses[-1] - metrics_callback.epoch_train_losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "Xtl9KmNfuf18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss and Accuracy\n",
        "plot_loss(metrics_callback.epoch_train_losses, metrics_callback.val_losses)"
      ],
      "metadata": {
        "id": "uWBVtKhDFkkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(metrics_data, \"/content/drive/MyDrive/Research_folder/outputs/metrics/size/XLM-R-75-metrics.json\") # update path"
      ],
      "metadata": {
        "id": "_VluOjlgG40C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/Research_folder/models/XLM-R-75-metrics\")  # Save the trained model # update path"
      ],
      "metadata": {
        "id": "cbP5e4ztC1WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation process"
      ],
      "metadata": {
        "id": "_9gxpbBKJztj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Remove leading whitespace from questions to avoid unnecessary token space usage\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize questions and contexts while handling long texts with a sliding window (stride)\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],  # Choose question or context based on padding side\n",
        "        examples[\"context\" if pad_on_right else \"question\"],  # Opposite choice for the second input\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",  # Truncate only the context (or question if needed)\n",
        "        max_length=max_length,  # Set max sequence length\n",
        "        stride=doc_stride,  # Define overlap between chunks for long contexts\n",
        "        return_overflowing_tokens=True,  # Return multiple input chunks for long contexts\n",
        "        return_offsets_mapping=True,  # Keep track of token positions in original text\n",
        "        padding=\"max_length\",  # Ensure all sequences have the same length\n",
        "    )\n",
        "\n",
        "    # Map each generated tokenized example to its original sample index\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Create a list to store the example ID corresponding to each tokenized input\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):  # Iterate through tokenized samples\n",
        "        # Identify whether the sequence contains context (1) or question (0)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0  # Set correct index based on padding side\n",
        "\n",
        "        # Find the original example index that this tokenized input corresponds to\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])  # Store example ID\n",
        "\n",
        "        # Adjust offset mapping: Set offsets to None for non-context tokens\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (offset if sequence_ids[k] == context_index else None)  # Keep offsets only for context tokens\n",
        "            for k, offset in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "UEwyeHYW_riJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_features = valid_dataset.map(\n",
        "    prepare_validation_features,  # Preprocess validation data\n",
        "    batched=True,  # Process the dataset in batches\n",
        "    remove_columns=valid_dataset.column_names  # Removes original columns\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:27:14.114273Z",
          "iopub.execute_input": "2021-09-21T17:27:14.114552Z",
          "iopub.status.idle": "2021-09-21T17:27:18.341925Z",
          "shell.execute_reply.started": "2021-09-21T17:27:14.114523Z",
          "shell.execute_reply": "2021-09-21T17:27:18.341253Z"
        },
        "trusted": true,
        "id": "Ylgu7yuqWBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of processed validation features\n",
        "print(f\"Number of validation features: {len(validation_features)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:27:20.649384Z",
          "iopub.execute_input": "2021-09-21T17:27:20.649632Z",
          "iopub.status.idle": "2021-09-21T17:27:20.655404Z",
          "shell.execute_reply.started": "2021-09-21T17:27:20.649604Z",
          "shell.execute_reply": "2021-09-21T17:27:20.654364Z"
        },
        "trusted": true,
        "id": "XdA0k8I_WBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:27:21.969179Z",
          "iopub.execute_input": "2021-09-21T17:27:21.969966Z",
          "iopub.status.idle": "2021-09-21T17:27:21.978611Z",
          "shell.execute_reply.started": "2021-09-21T17:27:21.969929Z",
          "shell.execute_reply": "2021-09-21T17:27:21.977952Z"
        },
        "trusted": true,
        "id": "QjmPIgciWBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smaller version of the validation features dataset\n",
        "valid_feats_small = validation_features.map(\n",
        "    lambda example: example,  # Lambda function returns the example\n",
        "    remove_columns=['example_id', 'offset_mapping']  # Remove unnecessary columns for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "TdlsV3bGDRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_feats_small"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:27:24.079482Z",
          "iopub.execute_input": "2021-09-21T17:27:24.079784Z",
          "iopub.status.idle": "2021-09-21T17:27:25.574520Z",
          "shell.execute_reply.started": "2021-09-21T17:27:24.079750Z",
          "shell.execute_reply": "2021-09-21T17:27:25.573780Z"
        },
        "trusted": true,
        "id": "RS_1RMhcWBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation dataset\n",
        "raw_predictions = trainer.predict(valid_feats_small)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:27:29.079355Z",
          "iopub.execute_input": "2021-09-21T17:27:29.079909Z",
          "iopub.status.idle": "2021-09-21T17:28:05.246651Z",
          "shell.execute_reply.started": "2021-09-21T17:27:29.079854Z",
          "shell.execute_reply": "2021-09-21T17:28:05.245994Z"
        },
        "trusted": true,
        "id": "OCbXiyyEWBOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the raw predictions\n",
        "raw_predictions[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:30:56.994787Z",
          "iopub.execute_input": "2021-09-21T17:30:56.995087Z",
          "iopub.status.idle": "2021-09-21T17:30:57.005439Z",
          "shell.execute_reply.started": "2021-09-21T17:30:56.995056Z",
          "shell.execute_reply": "2021-09-21T17:30:57.004509Z"
        },
        "trusted": true,
        "id": "u6GmnVDaWBOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = valid_dataset\n",
        "features = validation_features\n",
        "\n",
        "# Map example_id to its index in the examples dataset\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "\n",
        "# Initialize defaultdict to store feature indices per example\n",
        "features_per_example = collections.defaultdict(list)\n",
        "\n",
        "# Populate the dictionary with feature indices corresponding to each example_id\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n"
      ],
      "metadata": {
        "id": "OqDdHdr5D6kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    # Map example to its corresponding feature indices\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "    print(f\"Post-processing {len(examples)} examples with {len(features)} features.\")\n",
        "\n",
        "    # Iterate over all examples\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Process each feature associated with the current example\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Track minimum null score (if needed)\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            min_null_score = max(min_null_score or float('-inf'), feature_null_score)\n",
        "\n",
        "            # Get top n start and end logits\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "            # Evaluate each possible answer span\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip invalid spans\n",
        "                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    # Extract the answer text\n",
        "                    start_char, end_char = offset_mapping[start_index][0], offset_mapping[end_index][1]\n",
        "                    valid_answers.append({\"score\": start_logits[start_index] + end_logits[end_index], \"text\": context[start_char:end_char]})\n",
        "\n",
        "        # Select the best answer\n",
        "        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0] if valid_answers else {\"text\": \"\", \"score\": 0.0}\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:31:04.714913Z",
          "iopub.execute_input": "2021-09-21T17:31:04.715190Z",
          "iopub.status.idle": "2021-09-21T17:31:04.731920Z",
          "shell.execute_reply.started": "2021-09-21T17:31:04.715162Z",
          "shell.execute_reply": "2021-09-21T17:31:04.731203Z"
        },
        "trusted": true,
        "id": "k8aHYgQzWBOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-process raw predictions to extract the final answers for each example in the validation dataset\n",
        "final_predictions = postprocess_qa_predictions(valid_dataset, validation_features, raw_predictions.predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:31:07.419389Z",
          "iopub.execute_input": "2021-09-21T17:31:07.420202Z",
          "iopub.status.idle": "2021-09-21T17:31:11.798552Z",
          "shell.execute_reply.started": "2021-09-21T17:31:07.420166Z",
          "shell.execute_reply": "2021-09-21T17:31:11.797846Z"
        },
        "trusted": true,
        "id": "ZvofA6bhWBOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with questions and their corresponding predicted answers\n",
        "prediction = pd.DataFrame([{\"questions\": x1['question'], \"pred_answer\": x2} for x1, x2 in zip(valid_dataset, [i for i in final_predictions.values()])])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:31:12.989619Z",
          "iopub.execute_input": "2021-09-21T17:31:12.990339Z",
          "iopub.status.idle": "2021-09-21T17:31:13.017106Z",
          "shell.execute_reply.started": "2021-09-21T17:31:12.990300Z",
          "shell.execute_reply": "2021-09-21T17:31:13.016296Z"
        },
        "trusted": true,
        "id": "RQcC7gOJWBOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming valid_dataset contains the original context, question, and true answer\n",
        "prediction = pd.DataFrame([\n",
        "    {\n",
        "        \"questions\": x1['question'],\n",
        "        \"pred_answer\": x2,\n",
        "        \"answer_text\": x1['answer_text']  # Add the true answer here\n",
        "    }\n",
        "    for x1, x2 in zip(valid_dataset, [i for i in final_predictions.values()])\n",
        "])"
      ],
      "metadata": {
        "id": "gL_mJ30H9dWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataframe with questions predicted answer\n",
        "prediction"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-21T17:31:14.339372Z",
          "iopub.execute_input": "2021-09-21T17:31:14.339640Z",
          "iopub.status.idle": "2021-09-21T17:31:14.361615Z",
          "shell.execute_reply.started": "2021-09-21T17:31:14.339610Z",
          "shell.execute_reply": "2021-09-21T17:31:14.360613Z"
        },
        "trusted": true,
        "id": "WQtkjuybWBOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metric"
      ],
      "metadata": {
        "id": "1cEI9XVQKBtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove punctuation from a string\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "kyKxDezaAS6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Exact Match\n",
        "def compute_exact_match(pred, truth):\n",
        "    pred = remove_punctuation(pred.strip().lower())\n",
        "    truth = remove_punctuation(truth.strip().lower())\n",
        "    return int(pred == truth)"
      ],
      "metadata": {
        "id": "TgUiZXMBARe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute F1 Score\n",
        "def compute_f1(pred, truth):\n",
        "    # Remove punctuation from both pred and truth\n",
        "    pred = remove_punctuation(pred.strip().lower())\n",
        "    truth = remove_punctuation(truth.strip().lower())\n",
        "\n",
        "    # Split the text into words for F1 calculation\n",
        "    pred_tokens = pred.split()\n",
        "    truth_tokens = truth.split()\n",
        "\n",
        "    # Calculate the number of common tokens between the prediction and the truth\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2 * (precision * recall) / (precision + recall)"
      ],
      "metadata": {
        "id": "Btn9k4yyAMpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_xlm_r(text):\n",
        "    \"\"\"Tokenize Sinhala text using XLM-R tokenizer and join tokens.\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return \" \".join(tokens)  # Join tokens with space for ROUGE"
      ],
      "metadata": {
        "id": "qYY9sAc1GmOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge2\", \"rouge3\", \"rouge4\", \"rougeL\"], use_stemmer=False)\n",
        "\n",
        "def compute_rouge(pred, truth):\n",
        "    \"\"\"Compute ROUGE-2 and ROUGE-L scores for a given prediction and truth.\"\"\"\n",
        "    pred_tokens = tokenize_xlm_r(pred)\n",
        "    truth_tokens = tokenize_xlm_r(truth)\n",
        "    scores = scorer.score(truth_tokens, pred_tokens)\n",
        "    return scores['rouge2'].fmeasure, scores['rouge3'].fmeasure, scores['rouge4'].fmeasure, scores['rougeL'].fmeasure"
      ],
      "metadata": {
        "id": "NOY4VZ0vGo8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits Sinhala text into graphemes using the `grapheme` library.\n",
        "def tokenize_graphemes(text):\n",
        "    return grapheme.graphemes(text)  # Returns list of graphemes"
      ],
      "metadata": {
        "id": "JXbrUlu1IEXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(pred, truth, n_gram):\n",
        "    # Tokenize the prediction and truth\n",
        "    pred_tokens = \" \".join(tokenize_graphemes(pred))\n",
        "    truth_tokens = [\" \".join(tokenize_graphemes(truth))]  # Must be a list\n",
        "\n",
        "    # Create BLEU scorer with proper settings\n",
        "    bleu_scorer = BLEU(\n",
        "        max_ngram_order=n_gram,\n",
        "        smooth_method=\"exp\",\n",
        "        effective_order=True  # This addresses the warnings\n",
        "    )\n",
        "    bleu_score = bleu_scorer.sentence_score(pred_tokens, truth_tokens)\n",
        "    return bleu_score.score"
      ],
      "metadata": {
        "id": "Js1B7O45IGH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add metric per model\n",
        "def update_metrics_csv(model_name, metrics_dict, csv_file):\n",
        "    \"\"\"\n",
        "    Update a CSV file with model metrics, preserving existing data.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model\n",
        "        metrics_dict (dict): Dictionary of metrics to save\n",
        "        csv_file (str): Path to CSV file\n",
        "    \"\"\"\n",
        "    # Create new DataFrame with current metrics\n",
        "    new_data = {\n",
        "        'model': [model_name],\n",
        "        'timestamp': [datetime.now().isoformat()],\n",
        "        **metrics_dict\n",
        "    }\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "\n",
        "    # If file exists, load and append new data\n",
        "    if Path(csv_file).exists():\n",
        "        existing_df = pd.read_csv(csv_file)\n",
        "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "    else:\n",
        "        updated_df = new_df\n",
        "\n",
        "    # Save to CSV\n",
        "    updated_df.to_csv(csv_file, index=False)\n",
        "    print(f\"Metrics saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "soYTeQUr3bTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation set evaluation"
      ],
      "metadata": {
        "id": "5DYbERfy9uhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Exact Match for each row in the DataFrame\n",
        "prediction['exact_match'] = prediction.apply(lambda row: compute_exact_match(row['pred_answer'], row['answer_text']), axis=1)\n",
        "exact_match_score = prediction['exact_match'].mean() # percentage\n",
        "\n",
        "# Calculate F1 Score for each row in the DataFrame\n",
        "prediction['f1_scores'] = prediction.apply(lambda row: compute_f1(row['pred_answer'], row['answer_text']), axis=1)\n",
        "f1_score_value = sum(prediction['f1_scores']) / len(prediction['f1_scores'])\n",
        "\n",
        "# Print the results test set\n",
        "print(f\"Exact Match Score for val set: {exact_match_score:.4f}\")\n",
        "print(f\"F1 Score for val set: {f1_score_value:.4f}\")"
      ],
      "metadata": {
        "id": "8DsBKm8I89YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE-L and ROUGE-2\n",
        "prediction[[\"rouge_2\",\"rouge_3\", \"rouge_4\", \"rouge_L\"]] = prediction.apply(\n",
        "    lambda row: compute_rouge(row['pred_answer'], row['answer_text']), axis=1, result_type=\"expand\"\n",
        ")\n",
        "avg_rouge_2 = prediction['rouge_2'].mean()\n",
        "avg_rouge_3 = prediction['rouge_3'].mean()\n",
        "avg_rouge_4 = prediction['rouge_4'].mean()\n",
        "avg_rouge_L = prediction['rouge_L'].mean()\n",
        "\n",
        "print(f\"Average ROUGE-2 for Val set: {avg_rouge_2:.4f}\")\n",
        "print(f\"Average ROUGE-3 for Val set: {avg_rouge_3:.4f}\")\n",
        "print(f\"Average ROUGE-4 for Val set: {avg_rouge_4:.4f}\")\n",
        "print(f\"Average ROUGE-L for Val set: {avg_rouge_L:.4f}\")"
      ],
      "metadata": {
        "id": "aeH_fkUxHKJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction['bleu_1'] = prediction.apply(lambda row: compute_bleu(row['pred_answer'], row['answer_text'], n_gram=1), axis=1)\n",
        "prediction['bleu_2'] = prediction.apply(lambda row: compute_bleu(row['pred_answer'], row['answer_text'], n_gram=2), axis=1)\n",
        "\n",
        "avg_bleu_1 = prediction['bleu_1'].mean()\n",
        "avg_bleu_2 = prediction['bleu_2'].mean()\n",
        "\n",
        "print(f\"Average BLEU-1: {avg_bleu_1:.4f}\")\n",
        "print(f\"Average BLEU-2: {avg_bleu_2:.4f}\")"
      ],
      "metadata": {
        "id": "bWCOb6xyHYTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'avg_bleu_1': avg_bleu_1,\n",
        "    'avg_bleu_2': avg_bleu_2,\n",
        "    'avg_rouge_2': avg_rouge_2,\n",
        "    'avg_rouge_3': avg_rouge_3,\n",
        "    'avg_rouge_4': avg_rouge_4,\n",
        "    'avg_rouge_L': avg_rouge_L,\n",
        "    'exact_match_score': exact_match_score,\n",
        "    'f1_score_value': f1_score_value\n",
        "}\n",
        "update_metrics_csv(\"XLM-R-75-metrics\", metrics, csv_file = \"/content/drive/MyDrive/Research_folder/outputs/Results/Main_Metric_Results_val.csv\") #update model"
      ],
      "metadata": {
        "id": "_ncXp4jA3Xur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}