{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch datasets rouge-score sacrebleu grapheme"
      ],
      "metadata": {
        "id": "Uf3pcUFpf3xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "import os\n",
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from rouge_score import rouge_scorer\n",
        "import grapheme\n",
        "import seaborn as sns\n",
        "from sacrebleu.metrics import BLEU\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import Trainer, TrainingArguments, default_data_collator, AutoTokenizer, AutoModelForQuestionAnswering, EarlyStoppingCallback\n",
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from transformers import TrainerCallback\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "eWiJhIuMfZV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "R91yc8rQfwmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Research_folder/data/test.csv')"
      ],
      "metadata": {
        "id": "gc5VduEzf0fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test = pd.read_csv('/content/drive/MyDrive/Research_folder/data/SEEDData.csv')"
      ],
      "metadata": {
        "id": "WzWs2JmktcHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "XJakYnGVthVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_1.head()"
      ],
      "metadata": {
        "id": "LCKsmIMktlVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTKHBJYia_Nj"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"/content/drive/MyDrive/Research_folder/models/XLM-R-75-metrics\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up configuration for the tokenization\n",
        "batch_size = 16\n",
        "max_length = 512\n",
        "doc_stride = 128  # Stride value to handle document overflow\n",
        "pad_on_right = tokenizer.padding_side == \"right\"  # Check if padding is applied on the right side"
      ],
      "metadata": {
        "id": "REyPWLShbVx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "metadata": {
        "id": "LBbWcRCvbhbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Remove leading whitespace from questions to avoid unnecessary token space usage\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize questions and contexts while handling long texts with a sliding window (stride)\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],  # Choose question or context based on padding side\n",
        "        examples[\"context\" if pad_on_right else \"question\"],  # Opposite choice for the second input\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",  # Truncate only the context (or question if needed)\n",
        "        max_length=max_length,  # Set max sequence length\n",
        "        stride=doc_stride,  # Define overlap between chunks for long contexts\n",
        "        return_overflowing_tokens=True,  # Return multiple input chunks for long contexts\n",
        "        return_offsets_mapping=True,  # Keep track of token positions in original text\n",
        "        padding=\"max_length\",  # Ensure all sequences have the same length\n",
        "    )\n",
        "\n",
        "    # Map each generated tokenized example to its original sample index\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Create a list to store the example ID corresponding to each tokenized input\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):  # Iterate through tokenized samples\n",
        "        # Identify whether the sequence contains context (1) or question (0)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0  # Set correct index based on padding side\n",
        "\n",
        "        # Find the original example index that this tokenized input corresponds to\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])  # Store example ID\n",
        "\n",
        "        # Adjust offset mapping: Set offsets to None for non-context tokens\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (offset if sequence_ids[k] == context_index else None)  # Keep offsets only for context tokens\n",
        "            for k, offset in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "hfxPDJJCfSzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    # Map example to its corresponding feature indices\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "    print(f\"Post-processing {len(examples)} examples with {len(features)} features.\")\n",
        "\n",
        "    # Iterate over all examples\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Process each feature associated with the current example\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Track minimum null score (if needed)\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            min_null_score = max(min_null_score or float('-inf'), feature_null_score)\n",
        "\n",
        "            # Get top n start and end logits\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "            # Evaluate each possible answer span\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip invalid spans\n",
        "                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    # Extract the answer text\n",
        "                    start_char, end_char = offset_mapping[start_index][0], offset_mapping[end_index][1]\n",
        "                    valid_answers.append({\"score\": start_logits[start_index] + end_logits[end_index], \"text\": context[start_char:end_char]})\n",
        "\n",
        "        # Select the best answer\n",
        "        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0] if valid_answers else {\"text\": \"\", \"score\": 0.0}\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "TEKT_5PcfPes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the test DataFrame to a Hugging Face Dataset and process the features for validation\n",
        "test_dataset = Dataset.from_pandas(test)\n",
        "\n",
        "test_features = test_dataset.map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "QtoE7jh7cGJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns ('example_id' and 'offset_mapping') from the test features\n",
        "test_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])"
      ],
      "metadata": {
        "id": "qYVE7wnbcIJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_feats_small"
      ],
      "metadata": {
        "id": "aJ8o5Dr7cMGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_DISABLED=True\n",
        "args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    label_names=[\"start_positions\", \"end_positions\"]\n",
        ")\n",
        "trainer = Trainer(model, args)"
      ],
      "metadata": {
        "id": "U11mtg7ziHtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "test_predictions = trainer.predict(test_feats_small)"
      ],
      "metadata": {
        "id": "3w_j9lh2cIgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the format of the test features dataset, retaining the original columns and format\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))"
      ],
      "metadata": {
        "id": "7c_-YrMpcMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-process the raw predictions on the test dataset to get the final answers\n",
        "final_test_predictions = postprocess_qa_predictions(test_dataset, test_features, test_predictions.predictions)"
      ],
      "metadata": {
        "id": "g-CaMWLicPmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the predicted answers to the test DataFrame\n",
        "test['PredictionString'] = test['id'].apply(lambda r: final_test_predictions[r])\n",
        "test.head()"
      ],
      "metadata": {
        "id": "bLsssZLHcRZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics"
      ],
      "metadata": {
        "id": "SYu8bmRPcmKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove punctuation from a string\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "ClcIh76FcS4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Exact Match\n",
        "def compute_exact_match(pred, truth):\n",
        "    pred = remove_punctuation(pred.strip().lower())\n",
        "    truth = remove_punctuation(truth.strip().lower())\n",
        "    return int(pred == truth)"
      ],
      "metadata": {
        "id": "Up8dOn-zc8pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute F1 Score\n",
        "def compute_f1(pred, truth):\n",
        "    # Remove punctuation from both pred and truth\n",
        "    pred = remove_punctuation(pred.strip().lower())\n",
        "    truth = remove_punctuation(truth.strip().lower())\n",
        "\n",
        "    # Split the text into words for F1 calculation\n",
        "    pred_tokens = pred.split()\n",
        "    truth_tokens = truth.split()\n",
        "\n",
        "    # Calculate the number of common tokens between the prediction and the truth\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common_tokens) / len(pred_tokens)\n",
        "    recall = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2 * (precision * recall) / (precision + recall)"
      ],
      "metadata": {
        "id": "bvTlEfyNeXDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard score\n",
        "def compute_jaccard(pred, truth):\n",
        "    # Normalize and remove punctuation\n",
        "    pred = remove_punctuation(pred.strip().lower())\n",
        "    truth = remove_punctuation(truth.strip().lower())\n",
        "\n",
        "    # Convert to token sets\n",
        "    pred_tokens = set(pred.split())\n",
        "    truth_tokens = set(truth.split())\n",
        "\n",
        "    # If both are empty\n",
        "    if not pred_tokens and not truth_tokens:\n",
        "        return 1.0\n",
        "\n",
        "    # If either is empty\n",
        "    if not pred_tokens or not truth_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = pred_tokens.intersection(truth_tokens)\n",
        "    union = pred_tokens.union(truth_tokens)\n",
        "\n",
        "    return len(intersection) / len(union)"
      ],
      "metadata": {
        "id": "zjNiViNNYT_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_xlm_r(text):\n",
        "    \"\"\"Tokenize Sinhala text using XLM-R tokenizer and join tokens.\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return \" \".join(tokens)  # Join tokens with space for ROUGE"
      ],
      "metadata": {
        "id": "vHqgOO9FeXvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge2\", \"rouge3\", \"rouge4\", \"rougeL\"], use_stemmer=False)\n",
        "\n",
        "def compute_rouge(pred, truth):\n",
        "    \"\"\"Compute ROUGE-2 and ROUGE-L scores for a given prediction and truth.\"\"\"\n",
        "    pred_tokens = tokenize_xlm_r(pred)\n",
        "    truth_tokens = tokenize_xlm_r(truth)\n",
        "    scores = scorer.score(truth_tokens, pred_tokens)\n",
        "    return scores['rouge2'].fmeasure, scores['rouge3'].fmeasure, scores['rouge4'].fmeasure, scores['rougeL'].fmeasure"
      ],
      "metadata": {
        "id": "tRSiGD9oebS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits Sinhala text into graphemes using the `grapheme` library.\n",
        "def tokenize_graphemes(text):\n",
        "    return grapheme.graphemes(text)  # Returns list of graphemes"
      ],
      "metadata": {
        "id": "U-uv_jsXed5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(pred, truth, n_gram):\n",
        "    # Tokenize the prediction and truth\n",
        "    pred_tokens = \" \".join(tokenize_graphemes(pred))\n",
        "    truth_tokens = [\" \".join(tokenize_graphemes(truth))]  # Must be a list\n",
        "\n",
        "    # Create BLEU scorer with proper settings\n",
        "    bleu_scorer = BLEU(\n",
        "        max_ngram_order=n_gram,\n",
        "        smooth_method=\"exp\",\n",
        "        effective_order=True  # This addresses the warnings\n",
        "    )\n",
        "    bleu_score = bleu_scorer.sentence_score(pred_tokens, truth_tokens)\n",
        "    return bleu_score.score"
      ],
      "metadata": {
        "id": "h6eA91Wpef__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add metric per model\n",
        "def update_metrics_csv(model_name, metrics_dict, csv_file):\n",
        "    \"\"\"\n",
        "    Update a CSV file with model metrics, preserving existing data.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model\n",
        "        metrics_dict (dict): Dictionary of metrics to save\n",
        "        csv_file (str): Path to CSV file\n",
        "    \"\"\"\n",
        "    # Create new DataFrame with current metrics\n",
        "    new_data = {\n",
        "        'model': [model_name],\n",
        "        'timestamp': [datetime.now().isoformat()],\n",
        "        **metrics_dict\n",
        "    }\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "\n",
        "    # If file exists, load and append new data\n",
        "    if Path(csv_file).exists():\n",
        "        existing_df = pd.read_csv(csv_file)\n",
        "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "    else:\n",
        "        updated_df = new_df\n",
        "\n",
        "    # Save to CSV\n",
        "    updated_df.to_csv(csv_file, index=False)\n",
        "    print(f\"Metrics saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "fOls2Kfbejhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Exact Match for each row in the DataFrame\n",
        "test['exact_match'] = test.apply(lambda row: compute_exact_match(row['PredictionString'], row['answer_text']), axis=1)\n",
        "exact_match_score = test['exact_match'].mean() # percentage\n",
        "\n",
        "# Calculate F1 Score for each row in the DataFrame\n",
        "test['f1_scores'] = test.apply(lambda row: compute_f1(row['PredictionString'], row['answer_text']), axis=1)\n",
        "f1_score_value = sum(test['f1_scores']) / len(test['f1_scores'])\n",
        "\n",
        "# Apply row-wise Jaccard score\n",
        "test['jaccard_scores'] = test.apply(lambda row: compute_jaccard(row['PredictionString'], row['answer_text']), axis=1)\n",
        "jaccard_score_value = test['jaccard_scores'].mean()\n",
        "\n",
        "\n",
        "# Print the results test set\n",
        "print(f\"Exact Match Score for test set: {exact_match_score:.4f}\")\n",
        "print(f\"F1 Score for test set: {f1_score_value:.4f}\")\n",
        "print(f'Jaccard scores: {jaccard_score_value:.4f}')"
      ],
      "metadata": {
        "id": "toIk6xDKemSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE-L and ROUGE-2\n",
        "test[[\"rouge_2\",\"rouge_3\", \"rouge_4\", \"rouge_L\"]] = test.apply(\n",
        "    lambda row: compute_rouge(row['PredictionString'], row['answer_text']), axis=1, result_type=\"expand\"\n",
        ")\n",
        "avg_rouge_2 = test['rouge_2'].mean()\n",
        "avg_rouge_3 = test['rouge_3'].mean()\n",
        "avg_rouge_4 = test['rouge_4'].mean()\n",
        "avg_rouge_L = test['rouge_L'].mean()\n",
        "\n",
        "print(f\"Average ROUGE-2 for Val set: {avg_rouge_2:.4f}\")\n",
        "print(f\"Average ROUGE-3 for Val set: {avg_rouge_3:.4f}\")\n",
        "print(f\"Average ROUGE-4 for Val set: {avg_rouge_4:.4f}\")\n",
        "print(f\"Average ROUGE-L for Val set: {avg_rouge_L:.4f}\")"
      ],
      "metadata": {
        "id": "Tre5fD1yerh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BLEU-1 and BLEU-2\n",
        "test['bleu_1'] = test.apply(lambda row: compute_bleu(row['PredictionString'], row['answer_text'], n_gram=1), axis=1)\n",
        "test['bleu_2'] = test.apply(lambda row: compute_bleu(row['PredictionString'], row['answer_text'], n_gram=2), axis=1)\n",
        "\n",
        "avg_bleu_1 = test['bleu_1'].mean()\n",
        "avg_bleu_2 = test['bleu_2'].mean()\n",
        "\n",
        "# Print the results test set\n",
        "print(f\"Average BLEU-1 for Test set: {avg_bleu_1:.4f}\")\n",
        "print(f\"Average BLEU-2 for Test set: {avg_bleu_2:.4f}\")"
      ],
      "metadata": {
        "id": "fpNW56-Reu2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'avg_bleu_1': avg_bleu_1,\n",
        "    'avg_bleu_2': avg_bleu_2,\n",
        "    'avg_rouge_2': avg_rouge_2,\n",
        "    'avg_rouge_3': avg_rouge_3,\n",
        "    'avg_rouge_4': avg_rouge_4,\n",
        "    'avg_rouge_L': avg_rouge_L,\n",
        "    'exact_match_score': exact_match_score,\n",
        "    'f1_score_value': f1_score_value,\n",
        "    'jaccard_score_value': jaccard_score_value\n",
        "}\n",
        "update_metrics_csv(\"XLM-R-75\", metrics, csv_file = \"/content/drive/MyDrive/Research_folder/outputs/Results/Evaluation_Results_Test.csv\") #update model"
      ],
      "metadata": {
        "id": "1qmsCrZGexqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}